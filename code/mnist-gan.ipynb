{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tfe = tf.contrib.eager\n",
    "tfs = tf.contrib.summary\n",
    "tfs_logger = tfs.record_summaries_every_n_global_steps\n",
    "\n",
    "import sonnet as snt\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(snt.AbstractModule):\n",
    "    \n",
    "    def __init__(self, name='mnist_discriminator'):\n",
    "        \n",
    "        super(Discriminator, self).__init__(name=name)\n",
    "        \n",
    "    \n",
    "    def _build(self, inputs):\n",
    "        \n",
    "        # --------------------------------------------\n",
    "        # Define layers\n",
    "        # --------------------------------------------\n",
    "        \n",
    "        conv1 = snt.Conv2D(output_channels=64,\n",
    "                           kernel_shape=(5, 5),\n",
    "                           stride=(1, 1),\n",
    "                           padding='VALID')\n",
    "        \n",
    "        conv2 = snt.Conv2D(output_channels=64,\n",
    "                           kernel_shape=(5, 5),\n",
    "                           stride=(1, 1),\n",
    "                           padding='VALID')\n",
    "        \n",
    "        flatten = snt.BatchFlatten()\n",
    "        \n",
    "        fc = snt.Linear(output_size=1)\n",
    "        \n",
    "        # --------------------------------------------\n",
    "        # Apply layers\n",
    "        # --------------------------------------------\n",
    "        \n",
    "        activations = tf.nn.relu(conv1(inputs))\n",
    "        activations = tf.nn.relu(conv2(activations))\n",
    "        activations = fc(flatten(activations))\n",
    "        \n",
    "        return activations\n",
    "    \n",
    "    \n",
    "\n",
    "class Generator(snt.AbstractModule):\n",
    "    \n",
    "    def __init__(self, num_inputs, name='mnist_generator'):\n",
    "        \n",
    "        super(Generator, self).__init__(name=name)\n",
    "        \n",
    "        self._num_inputs = num_inputs\n",
    "        self._is_training = True\n",
    "        \n",
    "    \n",
    "    def _build(self, inputs):\n",
    "        \n",
    "        # --------------------------------------------\n",
    "        # Define layers\n",
    "        # --------------------------------------------\n",
    "        \n",
    "        fc1 = snt.Linear(output_size=196)\n",
    "        bn1 = snt.BatchNorm()\n",
    "        \n",
    "        fc2 = snt.Linear(output_size=196)\n",
    "        bn2 = snt.BatchNorm()\n",
    "        \n",
    "        reshape = snt.BatchReshape((14, 14, 1))\n",
    "        \n",
    "        deconv1 = snt.Conv2DTranspose(output_channels=32,\n",
    "                                      output_shape=(28, 28),\n",
    "                                      kernel_shape=(5, 5),\n",
    "                                      stride=(2, 2),\n",
    "                                      padding='SAME')\n",
    "        bn3 = snt.BatchNorm()\n",
    "        \n",
    "        deconv2 = snt.Conv2DTranspose(output_channels=1,\n",
    "                                      output_shape=(28, 28),\n",
    "                                      kernel_shape=(5, 5),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding='SAME')\n",
    "        \n",
    "        # --------------------------------------------\n",
    "        # Apply layers\n",
    "        # --------------------------------------------\n",
    "        \n",
    "        activations = bn1(tf.nn.leaky_relu(fc1(inputs)), is_training=self._is_training)\n",
    "        activations = bn2(tf.nn.leaky_relu(fc2(activations)), is_training=self._is_training)\n",
    "        activations = reshape(activations)\n",
    "        \n",
    "        activations = bn3(tf.nn.leaky_relu(deconv1(activations)), is_training=self._is_training)\n",
    "        activations = tf.nn.sigmoid(deconv2(activations))\n",
    "        \n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_input_fn(data, batch_size, shuffle_buffer=5000):\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.map(mnist_process_data)\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "    \n",
    "def mnist_process_data(image):\n",
    "    \n",
    "    processed = tf.cast(image, tf.float32) / 255\n",
    "    \n",
    "    return processed[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "num_epochs = 50\n",
    "num_discriminator_steps = 1\n",
    "log_frequency = 10\n",
    "\n",
    "learn_rate = 1e-3\n",
    "\n",
    "num_inputs = 100\n",
    "batch_size = 256\n",
    "\n",
    "# Load data\n",
    "((train_data, _),\n",
    " (test_data, _)) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_dataset = mnist_input_fn(train_data, batch_size=batch_size)\n",
    "num_batches = len(train_data) // batch_size\n",
    "\n",
    "# Create networks\n",
    "disc = Discriminator()\n",
    "gen = Generator(num_inputs=num_inputs)\n",
    "\n",
    "# Define optimiser\n",
    "optimizer = tf.train.AdamOptimizer(learn_rate)\n",
    "\n",
    "# Define tensorflow summary stuff\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "log_dir = '/tmp/mnist_gan/log'\n",
    "writer = tfs.create_file_writer(log_dir)\n",
    "writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9b1fef165c41969ad96dccf07239b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=234), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/stratis/Documents/sbrml/GANs/gan-venv/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    k = 0\n",
    "    for batch in tqdm(train_dataset.take(num_batches), total=num_batches):\n",
    "        \n",
    "        # Increment global step\n",
    "        global_step.assign_add(1)\n",
    "        \n",
    "        with tfs_logger(log_frequency):\n",
    "\n",
    "            # Update discriminator\n",
    "            if k < num_discriminator_steps:\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    z = tf.random.normal(mean=0, stddev=1, shape=(batch_size, num_inputs))\n",
    "                    gen_imgs = gen(z)\n",
    "\n",
    "                    gen_pred = disc(gen_imgs)\n",
    "                    true_pred = disc(batch)\n",
    "\n",
    "                    gen_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(gen_pred),\n",
    "                                                                       logits=gen_pred)\n",
    "\n",
    "                    true_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(true_pred),\n",
    "                                                                        logits=true_pred)\n",
    "\n",
    "                    loss = tf.reduce_mean(gen_loss) + tf.reduce_mean(true_loss)\n",
    "\n",
    "                grads = tape.gradient(loss, disc.get_all_variables())\n",
    "                optimizer.apply_gradients(zip(grads, disc.get_all_variables()))\n",
    "                \n",
    "                tfs.scalar('Discriminator_Loss', loss)\n",
    "\n",
    "                k += 1\n",
    "\n",
    "            # Update generator\n",
    "            else:\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    z = tf.random.normal(mean=0, stddev=1, shape=(batch_size, num_inputs))\n",
    "                    gen_imgs = gen(z)\n",
    "\n",
    "                    gen_pred = disc(gen_imgs)\n",
    "\n",
    "                    gen_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(gen_pred),\n",
    "                                                                       logits=gen_pred)\n",
    "\n",
    "                    loss = tf.reduce_mean(gen_loss)\n",
    "\n",
    "                grads = tape.gradient(loss, gen.get_all_variables())\n",
    "                optimizer.apply_gradients(zip(grads, gen.get_all_variables()))\n",
    "                \n",
    "                tfs.scalar('Generator_Loss', loss)\n",
    "\n",
    "                k = 0\n",
    "            \n",
    "            tfs.image('Generated_images', gen_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
